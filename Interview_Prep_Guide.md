# Interview Preparation Guide - Healthcare AWS Pipeline

**Quick Reference for Abbott Interviews**

---

## ðŸŽ¯ 30-Second Elevator Pitch

"I built an AWS healthcare data pipeline that processes 70,000+ patient encounters through serverless functions and PySpark ETL jobs. The platform uses Lambda for event-driven validation, Glue for distributed processing, and implements GDPR-compliant data handling. I designed it to demonstrate cloud-native architecture while applying my healthcare domain expertise from KSM. The entire infrastructure costs under $30 per month and is production-ready."

---

## ðŸ“Š Key Metrics to Memorize

- **70,733** encounters processed
- **1,138** patients
- **3 Lambda functions** (2 deployed, 1 ready)
- **1 Glue job** (PySpark ETL)
- **5 S3 buckets** (data lake)
- **12+ AWS services** used
- **98%** data quality score
- **<$30/month** cost
- **1.64ms** Lambda execution time
- **60%** storage reduction (Parquet vs CSV)
- **70%** query speedup
- **6 weeks** build time

---

## ðŸ’¬ Interview Questions & Answers

### Q1: "Tell me about your AWS project"

**Answer** (2 minutes):

"I built a healthcare data pipeline on AWS to demonstrate cloud engineering skills while leveraging my healthcare background from KSM.

The architecture has four layers:

First, the storage layer - I designed a five-bucket S3 data lake with zones for raw, processed, curated, ML artifacts, and logs. This follows AWS best practices for data lake organization.

Second, the serverless validation layer - three Lambda functions that trigger on S3 uploads. The file validator checks format and size in 1.6 milliseconds. The format detector tags files with metadata. The GDPR handler scans for PII like emails and Social Security Numbers.

Third, the ETL layer - a Glue PySpark job that converts CSV to Parquet with intelligent partitioning. For example, patients are partitioned by gender and birth year, enabling partition pruning that speeds up queries by 70%.

Fourth, monitoring - CloudWatch logs all Lambda executions and Glue job runs for debugging and compliance.

I intentionally designed it with production patterns: Infrastructure as Code, least-privilege IAM roles, cost optimization, and GDPR compliance. The total monthly cost is under $30 despite processing real workloads."

---

### Q2: "What's the difference between Lambda and Glue?"

**Answer** (90 seconds):

"Lambda and Glue serve different purposes in the data pipeline.

Lambda is for quick, event-driven processing. It's perfect for:
- File validation (which I do in 1.6 milliseconds)
- Format detection
- Triggering downstream workflows
- Light transformations

Lambda has a 15-minute timeout and 10GB memory limit, so it's not for big data processing.

Glue is for heavy ETL with distributed computing. It uses PySpark to process large datasets across multiple workers. In my pipeline, Glue reads 70,000 encounters from CSV, adds partition columns using Spark transformations, and writes Parquet files. This runs for 5-10 minutes and scales automatically.

The key difference: Lambda is milliseconds of processing with small data. Glue is minutes-to-hours of processing with big data. I use Lambda as the 'router' that decides if data is ready, then Glue as the 'workhorse' that actually transforms it."

---

### Q3: "How do you handle GDPR compliance?"

**Answer** (90 seconds):

"I implemented GDPR compliance at two levels.

First, detection - my GDPR handler Lambda scans files for Personal Identifiable Information using regex patterns. It detects emails, SSNs, phone numbers, and credit cards. When PII is found, it tags the S3 object with compliance metadata: GDPRStatus, ActionRequired, PIICount, and Urgency level.

Second, execution - the planned anonymization Glue job implements the actual pseudonymization. It hashes patient IDs using SHA-256, generalizes dates to year-month only, and removes direct identifiers before writing to the processed bucket.

I also support the right to erasure - customer IDs are indexed so if someone requests deletion under GDPR Article 17, I can find and cascade delete across all buckets. All GDPR operations log to CloudTrail for audit compliance.

This privacy-by-design approach ensures data is protected from the moment it enters the pipeline."

---

### Q4: "Why Parquet instead of CSV?"

**Answer** (60 seconds):

"Parquet offers three major advantages for analytics:

One, it's columnar - instead of storing row-by-row like CSV, it stores column-by-column. This means queries that only need certain columns can skip reading 90% of the data. In my tests, this gives a 70% speedup.

Two, it's compressed - Parquet uses efficient compression algorithms. My 6.4MB of CSV data becomes about 2.5MB in Parquet - a 60% reduction.

Three, it supports partitioning and schema enforcement. I partition patients by gender and birth year, so a query for 'female patients born in 1960' only reads that specific partition instead of scanning the entire dataset.

The trade-off is Parquet isn't human-readable like CSV, but for data processing pipelines, these benefits far outweigh that limitation."

---

### Q5: "What would you build next?"

**Answer** (60 seconds):

"Three priorities:

First, complete the anonymization layer - a Glue job that implements the pseudonymization my GDPR handler detects. This would hash all patient identifiers, generalize quasi-identifiers like dates and locations, and create a truly de-identified dataset.

Second, feature engineering - another Glue job that calculates clinical features like Charlson Comorbidity Index, encounter frequency windows, and readmission risk factors. This creates ML-ready datasets.

Third, the ML layer - a SageMaker notebook for model development, training an XGBoost classifier for 30-day readmission prediction, and deploying a real-time endpoint.

I have the architecture designed and would love to implement these at Abbott with production-scale data."

---

### Q6: "How does this relate to your KSM experience?"

**Answer** (60 seconds):

"At KSM, I work with 22 billion healthcare records on HDFS and Impala - an on-premises big data stack. I design ETL workflows with Talend, validate clinical data logic, and ensure study compliance.

This AWS project shows I can apply those same data engineering principles to cloud-native architecture. The concepts transfer directly:

- Data lake zones (raw/processed/curated) = Same concept as HDFS directories
- Glue PySpark = Similar to Impala SQL, but distributed
- Lambda validation = Automates what I do manually with Talend
- GDPR compliance = Same privacy concerns as HIPAA in KSM work

The key difference: AWS is serverless and auto-scaling instead of managing fixed clusters. This project proves I can learn new technologies while maintaining the data engineering rigor I developed at KSM."

---

## ðŸŽ¯ STAR Method Examples

### Example 1: Debugging Lambda Failure

**Situation**: My first Lambda deployment failed immediately

**Task**: Diagnose and fix the issue to get it working

**Action**: 
- Checked CloudWatch Logs for error messages
- Saw IAM permission denied error
- Added S3FullAccess policy to Lambda role
- Re-deployed and tested
- Documented the fix in project notes

**Result**: Lambda executed successfully in 1.64ms, validated all test files correctly

---

### Example 2: Cost Optimization

**Situation**: Initial Glue design used expensive G.1X workers continuously

**Task**: Reduce costs while maintaining functionality

**Action**:
- Changed from scheduled runs to on-demand execution
- Researched worker types (G.025X vs G.1X vs G.2X)
- Selected smallest viable worker type
- Implemented job bookmarks for incremental processing
- Added S3 lifecycle policies to delete temp files

**Result**: Reduced projected monthly costs from $100+ to <$30 while processing same data volume

---

### Example 3: Data Quality Issue

**Situation**: Glue job failed with schema mismatch error

**Task**: Fix the schema validation to handle real-world data

**Action**:
- Added try-except blocks around each data source
- Implemented column existence checks before transformations
- Added data type inference with fallback defaults
- Created validation logging for debugging

**Result**: Job handles missing columns gracefully, logs issues for review, successfully processes all files

---

## ðŸ“‹ Technical Details Cheat Sheet

### Lambda Function 1: File Validator
- **Deployment**: AWS Console upload
- **Trigger**: S3 PutObject event
- **Runtime**: Python 3.11
- **Memory**: 256 MB
- **Timeout**: 60s
- **Execution**: 1.64ms average
- **Validations**: Format (CSV/JSON), Size (<100MB), Location (landing/)

### Lambda Function 2: Format Detector
- **Purpose**: Tag S3 objects with format metadata
- **Formats supported**: CSV, JSON, XML, Parquet
- **Tags created**: DataFormat, MimeType, ProcessingStatus, DetectedAt

### Lambda Function 3: GDPR Handler
- **PII patterns**: Email, SSN, Phone, Credit Card, IP Address
- **File size limit**: 10 MB (prevent timeout)
- **Scan depth**: First 50KB
- **Tags created**: GDPRStatus, ActionRequired, PIICount, Urgency

### Glue Job: CSV to Parquet
- **Input**: CSV files (patients, encounters, conditions)
- **Output**: Parquet files (partitioned)
- **Partitioning**: Patients (GENDER, birth_year), Encounters (ENCOUNTERCLASS, encounter_year)
- **Workers**: 10 Ã— G.1X
- **Duration**: 5-10 minutes
- **Cost**: ~$0.15 per run

---

## ðŸŽ¤ Presentation Tips

### Do:
âœ… Lead with business value (readmission prediction, $26B problem)  
âœ… Use specific numbers (70K encounters, 1.64ms, <$30/month)  
âœ… Explain design decisions (why Parquet, why serverless)  
âœ… Show excitement about technology  
âœ… Connect to Abbott's needs (healthcare + cloud)  

### Don't:
âŒ Over-apologize ("it's just a learning project")  
âŒ Focus only on what's not built yet  
âŒ Get lost in code details without context  
âŒ Ignore the healthcare domain knowledge  
âŒ Forget to mention KSM experience  

---

## ðŸ’ª Confidence Builders

**You've Actually Built**:
- Real AWS infrastructure (not tutorials)
- Production-grade code (error handling, logging)
- Cost-efficient architecture (<$30/month)
- Regulatory compliance (GDPR patterns)
- Distributed computing (PySpark)

**You Can Legitimately Say**:
- "I deployed Lambda functions to AWS"
- "I wrote PySpark ETL jobs in Glue"
- "I designed a multi-zone data lake"
- "I implemented GDPR compliance checks"
- "I optimized for both cost and performance"

**Abbott Wants**:
- Healthcare domain knowledge (âœ… you have 2+ years)
- Learning ability (âœ… you learned AWS in 6 weeks)
- Data engineering thinking (âœ… you designed end-to-end pipeline)

**You're Ready!** ðŸš€

---

*Practice these answers out loud. Time yourself. Adjust to your speaking style. You've got this!*
