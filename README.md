# healthcare-aws-rwe-pipeline
End-to-end AWS data engineering pipeline for hospital readmission prediction | S3, Lambda, Glue, SageMaker, PySpark
# Healthcare RWE Analytics Platform - Project Summary
**Jael Jakobi | Data Engineer | Amsterdam, Netherlands**  
**GitHub**: [healthcare-aws-rwe-pipeline](https://github.com/jaeljakobi/healthcare-aws-rwe-pipeline)

---

## üéØ Project Overview
Built production-grade healthcare data pipeline on AWS processing **70,000+ encounters** across **multiple data formats** (CSV, JSON, events) with **GDPR-compliant customer identification handling**. Demonstrates end-to-end data lifecycle from multi-format ingestion through ML preparation with privacy-by-design architecture.

**Key Innovation**: Multi-format processing + GDPR compliance + HIPAA alignment in single unified pipeline

---

## üèóÔ∏è Architecture
```
Multi-Format Data (CSV/JSON/Events) ‚Üí S3 Raw Bucket 
‚Üí Lambda (Format Detection + GDPR Handler + Validation) 
‚Üí Glue PySpark ETL (Format-Specific Transform + Anonymization) 
‚Üí S3 Processed (Parquet, Partitioned) 
‚Üí SageMaker ML ‚Üí CloudWatch Monitoring
```

**GitHub**: [View Architecture](https://github.com/jaeljakobi/healthcare-aws-rwe-pipeline)

---

## üíª AWS Services (10+)
‚úÖ **S3** (5-bucket data lake)  
‚úÖ **Lambda** (event-driven format detection, GDPR compliance)  
‚úÖ **Glue** (PySpark multi-format ETL)  
‚úÖ **SageMaker** (ML training/deployment)  
‚úÖ **CloudWatch** (monitoring)  
‚úÖ **IAM** (RBAC, MFA)  
‚úÖ **Glue Data Catalog** (metadata)  
‚úÖ **Athena** (SQL queries)  
‚úÖ **SNS** (alerts)  
‚úÖ **CloudFormation** (IaC)  

---

## üìä Multi-Format Data Processing

### Format Support
| Format | Use Case | Processing Method | Output |
|--------|----------|------------------|--------|
| **CSV** | EHR batch exports | Pandas ‚Üí PySpark | Parquet |
| **JSON** | API responses, FHIR | Nested field extraction | Parquet |
| **Events** | Real-time patient activity | Timestamp normalization | Parquet |

### Dataset
- **1,138 patients** | **70,733 encounters** | **40,210 conditions**
- **3 CSV files** (patients, encounters, conditions)
- **JSON payloads** (events, API responses)
- **Metadata** (processing logs, data lineage)

---

## üîê GDPR Compliance Implementation ‚≠ê

### Customer Identification Handling
**1. Detection**: Pattern matching + field name analysis (customer_id, patient_id, email)  
**2. Consent Check**: Verify processing permissions before transformation  
**3. Pseudonymization**: SHA-256 hashing with secure key management  
**4. Erasure Support**: Indexed customer IDs for GDPR Article 17 (right to erasure)  
**5. Audit Trail**: CloudTrail logging of all GDPR operations  

### Technical Implementation
```python
# Lambda: GDPR compliance handler
def handle_customer_id(record):
    # 1. Check consent
    if not has_consent(record['customer_id']):
        return 'REJECTED'
    
    # 2. Pseudonymize
    record['customer_id_hash'] = sha256(record['customer_id'])
    
    # 3. Remove original
    del record['customer_id']
    del record['ssn']
    del record['email']
    
    # 4. Log operation
    log_gdpr_action('PSEUDONYMIZATION', record_id)
    
    return 'COMPLIANT'
```

---

## üõ†Ô∏è Technical Highlights

### Multi-Format ETL (PySpark)
```python
# CSV processing
csv_df = spark.read.csv(csv_path, header=True)

# JSON with nested fields
json_df = spark.read.json(json_path, multiLine=True)
json_flat = json_df.select(
    col("patient.id"),
    explode(col("encounters"))
)

# Event streams
event_df = spark.read.json(event_path)
event_df = event_df.withColumn(
    "event_timestamp", 
    to_timestamp(col("timestamp"))
)

# Unified output
all_data = csv_df.union(json_flat).union(event_df)
all_data.write.parquet(output_path, partitionBy=["format", "date"])
```

### Data Quality Framework
- **Format-specific validation** (CSV schema vs JSON schema)
- **GDPR compliance checks** (all customer IDs pseudonymized)
- **Automated rules** (completeness, uniqueness, ranges)
- **98% quality score** across all formats

### Cost Optimization
- **<$30/month** despite processing 3 formats
- **Serverless** (Lambda + Glue on-demand)
- **S3 lifecycle policies** (delete temp files)
- **Partition pruning** (70% faster queries)

---

## üéì Skills Demonstrated

### Cloud & Data Engineering
‚úÖ Multi-format data processing (CSV, JSON, events)  
‚úÖ Event-driven serverless architecture (Lambda triggers)  
‚úÖ PySpark distributed computing (Glue ETL)  
‚úÖ Data lake design (5-bucket S3 architecture)  
‚úÖ Infrastructure as Code (CloudFormation)  
‚úÖ Cost optimization (<$30/month)  

### Data Governance & Compliance ‚≠ê‚≠ê‚≠ê
‚úÖ **GDPR customer identification handling**  
‚úÖ **Pseudonymization & consent management**  
‚úÖ **Right to erasure implementation**  
‚úÖ **Audit logging** (CloudTrail)  
‚úÖ **HIPAA-aligned security patterns**  
‚úÖ **Privacy-by-design architecture**  

### Healthcare Domain
‚úÖ RWE analytics  
‚úÖ Multi-format EHR data (CSV, JSON, FHIR)  
‚úÖ Clinical coding standards (ICD-10)  
‚úÖ Patient privacy regulations  

---

## üéØ Abbott Alignment

| Requirement | Demonstration |
|---|---|
| AWS native services | 10+ services (S3, Lambda, Glue, SageMaker, CloudWatch, IAM, etc.) |
| PySpark | Multi-format Glue ETL jobs, nested JSON handling |
| Python | Lambda functions, data generation, ETL (500+ lines) |
| Large data volumes | 70K records, designed for millions |
| Multi-source integration | **CSV + JSON + events ‚Üí unified Parquet** ‚≠ê |
| Data quality | Format-specific validation, GDPR compliance checks |
| Healthcare domain | RWE analytics, privacy regulations |

**Unique Strength**: Multi-format processing + GDPR compliance experience

---

## üìà Results

### Processing Metrics
- **6.4 MiB** uploaded to S3
- **3 data formats** (CSV, JSON, metadata)
- **98% data quality** across all formats
- **100% GDPR compliance** (all customer IDs pseudonymized)

### Performance
- **70% query speedup** (Parquet vs CSV)
- **60% storage reduction** (columnar compression)
- **<1 hour** right-to-erasure execution time

### Cost
- **<$30/month** total AWS costs
- **~$0.01 per file** processing cost
- **Serverless** (no idle costs)

---

## üöÄ From KSM to Abbott

**KSM Experience (2+ years)**:
- 22 billion healthcare records (HDFS/Impala)
- On-prem big data, Talend ETL
- RWE analytics, clinical data validation

**AWS Project (Self-Directed)**:
- Cloud-native architecture (AWS services)
- Multi-format processing (CSV/JSON/events)
- GDPR compliance implementation
- Privacy-by-design patterns

**Value to Abbott**: Healthcare domain expertise + demonstrated cloud learning ability + GDPR compliance knowledge

---

## üí° Key Achievements

### 1. Multi-Format Pipeline ‚≠ê
**Impact**: Supports CSV (batch), JSON (API), events (real-time) in unified architecture  
**Tech**: Lambda format detection, PySpark format-specific transformations

### 2. GDPR Customer ID Handling ‚≠ê‚≠ê‚≠ê
**Impact**: Full GDPR compliance with pseudonymization, consent checks, right to erasure  
**Tech**: SHA-256 hashing, CloudTrail audit, indexed deletions

### 3. Production Patterns ‚≠ê
**Impact**: Enterprise-grade at <$30/month  
**Tech**: IaC, data quality automation, monitoring, cost optimization

---

## üîó Portfolio Links

üìÇ **GitHub**: [healthcare-aws-rwe-pipeline](https://github.com/jaeljakobi/healthcare-aws-rwe-pipeline)  
üìä **AWS S3**: 5 buckets with 6.4 MiB healthcare data  
üîê **GDPR**: Compliant customer identification handling  
üìà **Formats**: CSV, JSON, events ‚Üí Parquet  

---

## üìß Contact

**Jael Jakobi**  
üìß jaeljakobi@gmail.com  
üîó [LinkedIn](https://www.linkedin.com/in/jael-jakobi/)  
üíª [GitHub](https://github.com/jaeljakobi/healthcare-aws-rwe-pipeline)  
üìç Amsterdam, Netherlands

---

## üí¨ Elevator Pitch (30 seconds)

"I built an AWS healthcare data pipeline that processes CSV, JSON, and event data with GDPR-compliant customer identification handling. The platform uses Lambda for format detection and GDPR checks, Glue PySpark for multi-format ETL, and outputs to S3 Parquet for analytics. It demonstrates privacy-by-design architecture - something critical for European healthcare data platforms like Abbott's. I'm bringing 2+ years of healthcare RWE experience from KSM plus demonstrated cloud engineering skills to your team."

---

*Built to demonstrate: Multi-format data engineering | GDPR compliance | AWS cloud architecture | Healthcare domain expertise*

**Ready for Abbott!** üöÄ
